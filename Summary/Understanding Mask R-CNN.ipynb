{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The studied Mask R-CNN model here is the default class `maskrcnn_resnet50_fpn` in `torchvision.mdoels.detection.mask_rcnn.py`. \n",
    "1. the backbone model is ResNet50;\n",
    "2. `Feature Pyramid Network` is employed between the backbone and the `Region Proposal Network`;\n",
    "3. Class `FastRCNNPredictor` is used to compute the class probability and bounding box prediction; class `MaskRCNNPredictor` is used to compute the mask fully convolutional network logits. \n",
    "\n",
    "<font color=red>The forward function in Faster R-CNN & Mask R-CNN is as follows:</font>\n",
    "```\n",
    "images, targets = self.transform(images, targets)\n",
    "features = self.backbone(images.tensors)\n",
    "if isinstance(features, torch.Tensor):\n",
    "    features = OrderedDict([('0', features)])\n",
    "proposals, proposal_losses = self.rpn(images, features, targets)\n",
    "detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
    "\n",
    "losses = {}\n",
    "losses.update(detector_losses)\n",
    "losses.update(proposal_losses)\n",
    "```\n",
    "\n",
    "The difference between Faster R-CNN and Mask R-CNN is that in Mask R-CNN, there are additional `mask_roi_pool, mask_head, mask_predictor`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel numbers in ResNet50: 3 -- 64 -- 256 -- 512 -- 1024 -- 2048. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pyramid Network\n",
    "The structure in the default Mask R-CNN model is as follows:\n",
    "\n",
    "<img src=\"../img/fpn_model.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal Network\n",
    "Region Proposal Network consists of two parts: \n",
    "1. anchor box generation: which uses the default `AnchorGenerator` class in `torchvision.models.detection.rpn.py`. \n",
    "2. anchor box selection: which uses class `RPNHead` in `torchvision.models.detection.rpn.py`. \n",
    "\n",
    "<img src=\"../img/RPN.png\" alt=\"drawing\" width=\"570\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AnchorGenerator(): to generate anchor boxes for each image\n",
    "1. default anchor sizes: ((32,), (64,), (128,), (256,), (512,))\n",
    "2. default aspect ratios: ((0.5, 1.0, 2.0),)\n",
    "\n",
    "Compute the cell anchors as:\n",
    "```\n",
    "(-23, -11, 23, 11), (-16, -16, 16, 16), (-11, -23, 11, 23)\n",
    "(-45, -23, 45, 23), (-32, -32, 32, 32), (-23, -45, 23, 45)\n",
    "(-91, -45, 91, 45), (-64, -64, 64, 64), (-45, -91, 45, 91)\n",
    "(-181, -91, 181, 91), (-128, -128, 128, 128), (-91, -181, 91, 181)\n",
    "(-362, -181, 362, 181), (-256, -256, 256, 256), (-181, -362, 181, 362)\n",
    "```\n",
    "The anchor sizes are in the pixel level of input images. So for each feature map, we need to compute its corresponding size.<br>\n",
    "Number of generated anchor boxes: $3*(200*272 + 100 * 136 + 50 * 68 + 25 *34 + 13 * 17) = 217413.$\n",
    "\n",
    "#### RoI selection based on anchor boxes, objectiveness and the offset\n",
    "1. based on the anchor boxes and predicted offset, get the corresponding RoIs;\n",
    "2. based on the objectiveness, select top `n` boxes independently per feature map before applying nms; (default `n=2000`. If number of anchor boxes in this level <= 2k, keep them all.)\n",
    "3. apply non-maximum suppression independently per level, only keep top `k` scoring predictions.\n",
    "<br>The whole process has nothing to do with the ground-truth bounding boxes. \n",
    "\n",
    "**The ground-truth bounding boxes are used when identifying the most matched anchor boxes with them**. \n",
    "From the comparision between the most matched anchor boxes with the ground-truth bounding boxes, we can evalute:\n",
    "1. the mismatch of objectivess;\n",
    "2. the mistatch of offset;\n",
    "<font color=red>They are computed as the loss and used to optimize the three convolutional layers in `RegionProposalNetwork`. </font>\n",
    "\n",
    "<font color=red>The loss in `RegionProposalNetwork` is the only factor determining the optimization of the three convolutional layers in `RegionProposalNetwork`. The remaining losses, like the final bounding box and object class prediction of each proposal box, are not relevant to the three convolutional layers in `RegionProposalNetwork`.</font>\n",
    "\n",
    "The answer is here: one code in `forward` function of `RegionProposalNetwork`:\n",
    "```\n",
    "# note that we detach the deltas because Faster R-CNN do not backprop through the proposals\n",
    "proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "```\n",
    "One good explanation of `detach` method of tensors: http://www.bnikolic.co.uk/blog/pytorch-detach.html\n",
    "\n",
    "<font color=red>The result from RPN is the selected proposal boxes, and if it is in the training mode, the corresponding loss between anchor boxes and ground-truth bounding boxes are computed and returned as well. </font>\n",
    "```\n",
    "proposals, proposal_losses = self.rpn(images, features, targets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code in `RPNHead` class, \n",
    "```\n",
    "self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1)\n",
    "self.bbox_pred = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1, stride=1)\n",
    "```\n",
    "From Feature Pyramid Network, the output is a list of tensors. They have the same number of channels, but different height and width. Now in the forward function of `RPNHead`, it is handled in the following way: \n",
    "```\n",
    "def forward(self, x):\n",
    "    # type: (List[Tensor])\n",
    "    logits = []\n",
    "    bbox_reg = []\n",
    "    for feature in x:\n",
    "        t = F.relu(self.conv(feature))\n",
    "        logits.append(self.cls_logits(t))\n",
    "        bbox_reg.append(self.bbox_pred(t))\n",
    "    return logits, bbox_reg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoIHeads class\n",
    "```\n",
    "(roi_heads): RoIHeads(\n",
    "(box_roi_pool): MultiScaleRoIAlign()\n",
    "(box_head): TwoMLPHead(\n",
    "  (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
    "  (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
    ")\n",
    "(box_predictor): FastRCNNPredictor(\n",
    "  (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
    "  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
    ")\n",
    "```\n",
    "Part of the forward function in class `RoIHeads`:\n",
    "```\n",
    "box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
    "box_features = self.box_head(box_features)\n",
    "class_logits, box_regression = self.box_predictor(box_features)\n",
    "```\n",
    "After getting the RoIs, further determine the level of feature maps based on equation in FPN paper. (Determine which FPN level each RoI in a set of RoIs should map to based on the heuristic in the FPN paper.)\n",
    "\n",
    "Then for the RoIs in the same level, convert them into features of the same size. \n",
    "```\n",
    "result_idx_in_level = roi_align(\n",
    "    per_level_feature, rois_per_level,\n",
    "    output_size=self.output_size,\n",
    "    spatial_scale=scale, sampling_ratio=self.sampling_ratio)\n",
    "```\n",
    "\n",
    "The called function is \n",
    "```\n",
    "torch.ops.torchvision.roi_align(input, rois, spatial_scale,\n",
    "                                           output_size[0], output_size[1],\n",
    "                                           sampling_ratio, aligned)\n",
    "```\n",
    "\n",
    "In the forward function, the offset between propoal boxes and ground-truth bounding boxes are computed in the following way:\n",
    "<img src=\"../img/ssd_loss.png\" alt=\"drawing\" width=\"550\"/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dive_into_deep_learning] *",
   "language": "python",
   "name": "conda-env-dive_into_deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
