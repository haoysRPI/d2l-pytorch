{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "Due to constraints on the length of this book, we cannot possibly introduce every single PyTorch function and class (and you probably would not want us to).\n",
    "\n",
    "## Finding all the functions and classes in the module\n",
    "\n",
    "In order to know which functions and classes can be called in a module, we invoke the `dir` function. For instance, we can query all properties in the `nd.random` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__text_signature__']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(dir(torch.tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we can ignore functions that start and end with `__` (special objects in Python) or functions that start with a single `_`(usually internal functions). Based on the remaining function/attribute names, we might hazard a guess that this module offers various methods for generating random numbers, including sampling from the uniform distribution (`uniform`), normal distribution (`normal`), and Poisson distribution  (`poisson`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the usage of specific functions and classes\n",
    "\n",
    "For more specific instructions on how to use a given function or class, we can invoke the  `help` function. As an example, let's explore the usage instructions for torch's `ones_like` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function ones_like:\n",
      "\n",
      "ones_like(...)\n",
      "    ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the same size as\n",
      "    :attr:`input`. ``torch.ones_like(input)`` is equivalent to\n",
      "    ``torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.\n",
      "    \n",
      "    .. warning::\n",
      "        As of 0.4, this function does not support an :attr:`out` keyword. As an alternative,\n",
      "        the old ``torch.ones_like(input, out=output)`` is equivalent to\n",
      "        ``torch.ones(input.size(), out=output)``.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the size of :attr:`input` will determine size of the output tensor\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n",
      "            Default: if ``None``, defaults to the dtype of :attr:`input`.\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned tensor.\n",
      "            Default: if ``None``, defaults to the layout of :attr:`input`.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, defaults to the device of :attr:`input`.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> input = torch.empty(2, 3)\n",
      "        >>> torch.ones_like(input)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.ones_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation, we can see that the `ones_like` function creates a new array with the same shape as the supplied tensor and all elements set to `1`. Whenever possible, you should run a quick test to confirm your interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0, 0, 0], [2, 2, 2]], dtype = torch.float)\n",
    "y = torch.ones_like(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Jupyter notebook, we can use `?` to display the document in another window. For example, `torch.randn?` will create content that is almost identical to `help(torch.randn)`, displaying it in a new browser window. In addition, if we use two question marks, e.g. `torch.randn??`, the code implementing the function will also be displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Documentation\n",
    "\n",
    "For further details on the API details check the PyTorch website at  [https://pytorch.org](https://pytorch.org). You can find the details under the appropriate headings (also for programming languages other than Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Look up `ones_like` and `autograd` in the API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torch.autograd in torch:\n",
      "\n",
      "NAME\n",
      "    torch.autograd\n",
      "\n",
      "DESCRIPTION\n",
      "    ``torch.autograd`` provides classes and functions implementing automatic\n",
      "    differentiation of arbitrary scalar valued functions. It requires minimal\n",
      "    changes to the existing code - you only need to declare :class:`Tensor` s\n",
      "    for which gradients should be computed with the ``requires_grad=True`` keyword.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _functions (package)\n",
      "    anomaly_mode\n",
      "    function\n",
      "    functional\n",
      "    grad_mode\n",
      "    gradcheck\n",
      "    profiler\n",
      "    variable\n",
      "\n",
      "CLASSES\n",
      "    torch._C._FunctionBase(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function._ContextMethodMixin, torch.autograd.function._HookMixin)\n",
      "    torch._C._LegacyVariableBase(builtins.object)\n",
      "        torch.autograd.variable.Variable\n",
      "    torch.autograd.function._ContextMethodMixin(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function._ContextMethodMixin, torch.autograd.function._HookMixin)\n",
      "    torch.autograd.function._HookMixin(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function._ContextMethodMixin, torch.autograd.function._HookMixin)\n",
      "    \n",
      "    class Function(torch._C._FunctionBase, _ContextMethodMixin, _HookMixin)\n",
      "     |  Records operation history and defines formulas for differentiating ops.\n",
      "     |  \n",
      "     |  Every operation performed on :class:`Tensor` s creates a new function\n",
      "     |  object, that performs the computation, and records that it happened.\n",
      "     |  The history is retained in the form of a DAG of functions, with edges\n",
      "     |  denoting data dependencies (``input <- output``). Then, when backward is\n",
      "     |  called, the graph is processed in the topological ordering, by calling\n",
      "     |  :func:`backward` methods of each :class:`Function` object, and passing\n",
      "     |  returned gradients on to next :class:`Function` s.\n",
      "     |  \n",
      "     |  Normally, the only way users interact with functions is by creating\n",
      "     |  subclasses and defining new operations. This is a recommended way of\n",
      "     |  extending torch.autograd.\n",
      "     |  \n",
      "     |  Examples::\n",
      "     |  \n",
      "     |      >>> class Exp(Function):\n",
      "     |      >>>\n",
      "     |      >>>     @staticmethod\n",
      "     |      >>>     def forward(ctx, i):\n",
      "     |      >>>         result = i.exp()\n",
      "     |      >>>         ctx.save_for_backward(result)\n",
      "     |      >>>         return result\n",
      "     |      >>>\n",
      "     |      >>>     @staticmethod\n",
      "     |      >>>     def backward(ctx, grad_output):\n",
      "     |      >>>         result, = ctx.saved_tensors\n",
      "     |      >>>         return grad_output * result\n",
      "     |      >>>\n",
      "     |      >>> #Use it by calling the apply method:\n",
      "     |      >>> output = Exp.apply(input)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Function\n",
      "     |      torch._C._FunctionBase\n",
      "     |      _ContextMethodMixin\n",
      "     |      _HookMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwargs)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  backward(ctx, *grad_outputs)\n",
      "     |      Defines a formula for differentiating the operation.\n",
      "     |      \n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      \n",
      "     |      It must accept a context :attr:`ctx` as the first argument, followed by\n",
      "     |      as many outputs did :func:`forward` return, and it should return as many\n",
      "     |      tensors, as there were inputs to :func:`forward`. Each argument is the\n",
      "     |      gradient w.r.t the given output, and each returned value should be the\n",
      "     |      gradient w.r.t. the corresponding input.\n",
      "     |      \n",
      "     |      The context can be used to retrieve tensors saved during the forward\n",
      "     |      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n",
      "     |      of booleans representing whether each input needs gradient. E.g.,\n",
      "     |      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n",
      "     |      first input to :func:`forward` needs gradient computated w.r.t. the\n",
      "     |      output.\n",
      "     |  \n",
      "     |  forward(ctx, *args, **kwargs)\n",
      "     |      Performs the operation.\n",
      "     |      \n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      \n",
      "     |      It must accept a context ctx as the first argument, followed by any\n",
      "     |      number of arguments (tensors or other types).\n",
      "     |      \n",
      "     |      The context can be used to store tensors that can be then retrieved\n",
      "     |      during the backward pass.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  is_traceable = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  apply(...) from torch.autograd.function.FunctionMeta\n",
      "     |  \n",
      "     |  register_hook(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  dirty_tensors\n",
      "     |  \n",
      "     |  metadata\n",
      "     |  \n",
      "     |  needs_input_grad\n",
      "     |  \n",
      "     |  next_functions\n",
      "     |  \n",
      "     |  non_differentiable\n",
      "     |  \n",
      "     |  requires_grad\n",
      "     |  \n",
      "     |  saved_tensors\n",
      "     |  \n",
      "     |  saved_variables\n",
      "     |  \n",
      "     |  to_save\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _ContextMethodMixin:\n",
      "     |  \n",
      "     |  mark_dirty(self, *args)\n",
      "     |      Marks given tensors as modified in an in-place operation.\n",
      "     |      \n",
      "     |      **This should be called at most once, only from inside the**\n",
      "     |      :func:`forward` **method, and all arguments should be inputs.**\n",
      "     |      \n",
      "     |      Every tensor that's been modified in-place in a call to :func:`forward`\n",
      "     |      should be given to this function, to ensure correctness of our checks.\n",
      "     |      It doesn't matter whether the function is called before or after\n",
      "     |      modification.\n",
      "     |  \n",
      "     |  mark_non_differentiable(self, *args)\n",
      "     |      Marks outputs as non-differentiable.\n",
      "     |      \n",
      "     |      **This should be called at most once, only from inside the**\n",
      "     |      :func:`forward` **method, and all arguments should be outputs.**\n",
      "     |      \n",
      "     |      This will mark outputs as not requiring gradients, increasing the\n",
      "     |      efficiency of backward computation. You still need to accept a gradient\n",
      "     |      for each output in :meth:`~Function.backward`, but it's always going to\n",
      "     |      be a zero tensor with the same shape as the shape of a corresponding\n",
      "     |      output.\n",
      "     |      \n",
      "     |      This is used e.g. for indices returned from a max :class:`Function`.\n",
      "     |  \n",
      "     |  mark_shared_storage(self, *pairs)\n",
      "     |  \n",
      "     |  save_for_backward(self, *tensors)\n",
      "     |      Saves given tensors for a future call to :func:`~Function.backward`.\n",
      "     |      \n",
      "     |      **This should be called at most once, and only from inside the**\n",
      "     |      :func:`forward` **method.**\n",
      "     |      \n",
      "     |      Later, saved tensors can be accessed through the :attr:`saved_tensors`\n",
      "     |      attribute. Before returning them to the user, a check is made to ensure\n",
      "     |      they weren't used in any in-place operation that modified their content.\n",
      "     |      \n",
      "     |      Arguments can also be ``None``.\n",
      "    \n",
      "    class Variable(torch._C._LegacyVariableBase)\n",
      "     |  Method resolution order:\n",
      "     |      Variable\n",
      "     |      torch._C._LegacyVariableBase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch._C._LegacyVariableBase:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "FUNCTIONS\n",
      "    backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\n",
      "        Computes the sum of gradients of given tensors w.r.t. graph leaves.\n",
      "        \n",
      "        The graph is differentiated using the chain rule. If any of ``tensors``\n",
      "        are non-scalar (i.e. their data has more than one element) and require\n",
      "        gradient, then the Jacobian-vector product would be computed, in this\n",
      "        case the function additionally requires specifying ``grad_tensors``.\n",
      "        It should be a sequence of matching length, that contains the \"vector\"\n",
      "        in the Jacobian-vector product, usually the gradient of the differentiated\n",
      "        function w.r.t. corresponding tensors (``None`` is an acceptable value for\n",
      "        all tensors that don't need gradient tensors).\n",
      "        \n",
      "        This function accumulates gradients in the leaves - you might need to zero\n",
      "        them before calling it.\n",
      "        \n",
      "        Arguments:\n",
      "            tensors (sequence of Tensor): Tensors of which the derivative will be\n",
      "                computed.\n",
      "            grad_tensors (sequence of (Tensor or None)): The \"vector\" in the Jacobian-vector\n",
      "                product, usually gradients w.r.t. each element of corresponding tensors.\n",
      "                None values can be specified for scalar Tensors or ones that don't require\n",
      "                grad. If a None value would be acceptable for all grad_tensors, then this\n",
      "                argument is optional.\n",
      "            retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n",
      "                will be freed. Note that in nearly all cases setting this option to ``True``\n",
      "                is not needed and often can be worked around in a much more efficient\n",
      "                way. Defaults to the value of ``create_graph``.\n",
      "            create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "                be constructed, allowing to compute higher order derivative products.\n",
      "                Defaults to ``False``.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Variable', 'Function', 'backward', 'grad_mode']\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/dive_into_deep_learning/lib/python3.6/site-packages/torch/autograd/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dive_into_deep_learning] *",
   "language": "python",
   "name": "conda-env-dive_into_deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
